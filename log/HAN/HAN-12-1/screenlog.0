dyhu@mit116:~/BDCI2017-MingLue/log/HAN/HAN-12-1$ [Kdyhu@mit116:~/BDCI2017-MingLue/log/HAN/HAN-12-1$ cd ../../..
dyhu@mit116:~/BDCI2017-MingLue$ CUDA_VISI LE[K[K[K[KIBLE_DEVICES=2 python train.py --model-id 4
Using TensorFlow backend.
loading data...
max sentence length:  42583
total vocab size 691360
load word2index
0 1
(120000, 40, 60)
data loaded
config vocab size: 194170
THCudaCheck FAIL file=/pytorch/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory
Traceback (most recent call last):
  File "train.py", line 214, in <module>
    main(args.model_id, use_element, args.is_save)
  File "train.py", line 96, in main
    model = model_selector(config, model_id, use_element)
  File "/home/dyhu/BDCI2017-MingLue/utils/trainhelper.py", line 37, in model_selector
    model = HAN(config)
  File "/home/dyhu/BDCI2017-MingLue/models/hierarchical.py", line 177, in __init__
    self.word_to_sentence = WordToSentence(config)
  File "/home/dyhu/BDCI2017-MingLue/models/hierarchical.py", line 24, in __init__
    self.word_context = nn.Parameter(torch.FloatTensor(config.word_context_size, 1).uniform_(-0.1, 0.1).cuda())  # TODO ÊîπÂèòÂàùÂßãÂåñÊñπÂºè
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/_utils.py", line 66, in _cuda
    return new_type(self.size()).copy_(self, async)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/cuda/__init__.py", line 269, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:66
dyhu@mit116:~/BDCI2017-MingLue$ CUDA_VISIBLE_DEVICES=2 python train.py --model-id 4[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ccd ../../..[K[7Pexit[K[K[K[Kclear
[H[Jdyhu@mit116:~/BDCI2017-MingLue$ clearCUDA_VISIBLE_DEVICES=2 python train.py --model-id 4
Using TensorFlow backend.
loading data...
max sentence length:  42583
total vocab size 691360
load word2index
0 1
(120000, 40, 60)
data loaded
config vocab size: 194170
THCudaCheck FAIL file=/pytorch/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory
Traceback (most recent call last):
  File "train.py", line 214, in <module>
    main(args.model_id, use_element, args.is_save)
  File "train.py", line 96, in main
    model = model_selector(config, model_id, use_element)
  File "/home/dyhu/BDCI2017-MingLue/utils/trainhelper.py", line 37, in model_selector
    model = HAN(config)
  File "/home/dyhu/BDCI2017-MingLue/models/hierarchical.py", line 177, in __init__
    self.word_to_sentence = WordToSentence(config)
  File "/home/dyhu/BDCI2017-MingLue/models/hierarchical.py", line 24, in __init__
    self.word_context = nn.Parameter(torch.FloatTensor(config.word_context_size, 1).uniform_(-0.1, 0.1).cuda())  # TODO ÊîπÂèòÂàùÂßãÂåñÊñπÂºè
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/_utils.py", line 66, in _cuda
    return new_type(self.size()).copy_(self, async)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/cuda/__init__.py", line 269, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:66
dyhu@mit116:~/BDCI2017-MingLue$ CUDA_VISIBLE_DEVICES=2 python train.py --model-id 4[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cclear[KCUDA_VISIBLE_DEVICES=2 python train.py --model-id 4[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ccd ../../..[K[7PexitCUDA_VISIBLE_DEVICES=3 python train.py --model-id 4[1P[C[C[C[C[C[C[C[C[C[C[C[C[8P[C[C[C[C[C[C[C[C[C[C[C[Ccd ..[K/..[4Pexit[K[K[K[Klsess log/rcnn-12-01.log [3@screen -r HAN-intermediate[Cls[Knvidia-smi [13@less log/rcnn-12-01.log[C[7Pscreen -r lhlilu[C[6Pnvidia-smi[C[13@less log/rcnn-12-01.log[C[13Pnvidia-smi[C[K[K[K[K[K[K[K[K[K[K[KCUDA_VISIBLE_DEVICES pythontr[K[K  [Ktrain.py --model-id 4
CUDA_VISIBLE_DEVICES: command not found
dyhu@mit116:~/BDCI2017-MingLue$ CUDA_VISIBLE_DEVICES python train.py --model-id 4[1@=[1@3
Using TensorFlow backend.
loading data...
max sentence length:  42583
total vocab size 691360
load word2index
0 1
(120000, 40, 60)
data loaded
config vocab size: 194170
pretrain...
loss weight: 
 0.7786
 0.8593
 1.1661
 1.2979
 1.1405
 0.9833
 0.5955
 1.1791
[torch.FloatTensor of size 8]

training...
lr: 0.001 lr2: 0.0
[1,    93] loss: 1.946, acc: 17.188
[1,   186] loss: 1.813, acc: 23.438
[1,   279] loss: 1.778, acc: 39.062
[1,   372] loss: 1.745, acc: 28.125
[1,   465] loss: 1.721, acc: 29.688
[1,   558] loss: 1.701, acc: 40.625
[1,   651] loss: 1.681, acc: 43.750
Terminated
dyhu@mit116:~/BDCI2017-MingLue$ CUDA_VISIBLE_DEVICES=3 python train.py --model-id 4[2P[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[2@=2[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[2P[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[2@=3[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[KCUDA_VISIBLE_DEVICES=3 python train.py --model-id 4
Using TensorFlow backend.
loading data...
max sentence length:  42583
total vocab size 691360
load word2index
0 1
(120000, 40, 60)
data loaded
config vocab size: 194170
pretrain...
loss weight: 
 0.7786
 0.8593
 1.1661
 1.2979
 1.1405
 0.9833
 0.5955
 1.1791
[torch.FloatTensor of size 8]

training...
lr: 0.001 lr2: 0.0
[1,    93] loss: 1.935, acc: 42.188
[1,   186] loss: 1.812, acc: 23.438
[1,   279] loss: 1.769, acc: 34.375
[1,   372] loss: 1.745, acc: 35.938
[1,   465] loss: 1.723, acc: 42.188
[1,   558] loss: 1.715, acc: 35.938
[1,   651] loss: 1.708, acc: 29.688
[1,   744] loss: 1.677, acc: 34.375
[1,   837] loss: 1.665, acc: 35.938
[1,   930] loss: 1.646, acc: 37.500
[1,  1023] loss: 1.662, acc: 40.625
[1,  1116] loss: 1.640, acc: 35.938
[1,  1209] loss: 1.634, acc: 28.125
[1,  1302] loss: 1.612, acc: 29.688
[1,  1395] loss: 1.605, acc: 39.062
[1,  1488] loss: 1.605, acc: 40.625
predicting...
THCudaCheck FAIL file=/pytorch/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory
Traceback (most recent call last):
  File "train.py", line 214, in <module>
    main(args.model_id, use_element, args.is_save)
  File "train.py", line 174, in main
    loss_weight = do_eval(valid_loader, model, model_id, config.has_cuda)
  File "/home/dyhu/BDCI2017-MingLue/utils/trainhelper.py", line 78, in do_eval
    outputs = model(Variable(texts))
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/nn/modules/module.py", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/dyhu/BDCI2017-MingLue/models/hierarchical.py", line 249, in forward
    x = self.word_to_sentence(x, word_hidden_stat, sequence_lens)  # [batch_size * num_sentences, word_hidden_size*2]
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/nn/modules/module.py", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/dyhu/BDCI2017-MingLue/models/hierarchical.py", line 69, in forward
    output, _ = self.word_to_sentence(packed, word_hidden_stat)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/nn/modules/module.py", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 162, in forward
    output, hidden = func(input, self.all_weights, hx)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/nn/_functions/rnn.py", line 351, in forward
    return func(input, *fargs, **fkwargs)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/autograd/function.py", line 284, in _do_forward
    flat_output = super(NestedIOFunction, self)._do_forward(*flat_input)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/autograd/function.py", line 306, in forward
    result = self.forward_extended(*nested_tensors)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/nn/_functions/rnn.py", line 293, in forward_extended
    cudnn.rnn.forward(self, input, hx, weight, output, hy)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/backends/cudnn/rnn.py", line 291, in forward
    fn.reserve = torch.cuda.ByteTensor(reserve_size.value)
RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:66
dyhu@mit116:~/BDCI2017-MingLue$ [Kdyhu@mit116:~/BDCI2017-MingLue$ CUDA_VISIBLE_DEVICES=3 python train.py --model-id 4[2P[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[2@=2[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cclear[KCUDA_VISIBLE_DEVICES=2 python train.py --model-id 4[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ccd ../../..[K[7PexitCUDA_VISIBLE_DEVICES=3 python train.py --model-id 4[1P[C[C[C[C[C[C[C[C[C[C[C[C[8P[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ccd ..[K/..[Klsess log/rcnn-12-01.log [3@screen -r HAN-intermediate[Cls[Knvidia-smi [13@less log/rcnn-12-01.log[C[7Pscreen -r lhlilu[C[6Pnvidia-smi[C[13@less log/rcnn-12-01.log[C[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Ksclea[K[K[K[K[Kclear
[H[Jdyhu@mit116:~/BDCI2017-MingLue$ CUDA_VISIBLE_DEVICES=2 python tre[K[K[K[K[K[K[K[Ktho [Kn results/[K[K[K[K[K[K[K[Ktrain.py --model-id 4
Using TensorFlow backend.
loading data...
max sentence length:  42583
total vocab size 691360
load word2index
0 1
(120000, 40, 60)
data loaded
config vocab size: 194170
pretrain...
loss weight: 
 0.7786
 0.8593
 1.1661
 1.2979
 1.1405
 0.9833
 0.5955
 1.1791
[torch.FloatTensor of size 8]

training...
lr: 0.001 lr2: 0.0
[1,    93] loss: 1.942, acc: 23.438
[1,   186] loss: 1.805, acc: 26.562
[1,   279] loss: 1.761, acc: 34.375
[1,   372] loss: 1.742, acc: 37.500
[1,   465] loss: 1.729, acc: 26.562
[1,   558] loss: 1.693, acc: 29.688
[1,   651] loss: 1.677, acc: 31.250
[1,   744] loss: 1.681, acc: 39.062
[1,   837] loss: 1.657, acc: 48.438
[1,   930] loss: 1.655, acc: 37.500
[1,  1023] loss: 1.668, acc: 28.125
[1,  1116] loss: 1.632, acc: 32.812
[1,  1209] loss: 1.626, acc: 37.500
[1,  1302] loss: 1.614, acc: 39.062
[1,  1395] loss: 1.602, acc: 45.312
[1,  1488] loss: 1.589, acc: 39.062
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[5, 1, 5, 1, 1, 5, 1, 2, 5, 2]
Acc: 40.2166666667
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.373653320793699
lr: 0.001 lr2: 0.0
[2,    93] loss: 1.565, acc: 43.750
[2,   186] loss: 1.579, acc: 39.062
[2,   279] loss: 1.574, acc: 34.375
[2,   372] loss: 1.570, acc: 45.312
[2,   465] loss: 1.586, acc: 35.938
[2,   558] loss: 1.565, acc: 32.812
[2,   651] loss: 1.563, acc: 42.188
[2,   744] loss: 1.564, acc: 45.312
[2,   837] loss: 1.551, acc: 35.938
[2,   930] loss: 1.563, acc: 46.875
[2,  1023] loss: 1.549, acc: 46.875
[2,  1116] loss: 1.530, acc: 50.000
[2,  1209] loss: 1.536, acc: 45.312
[2,  1302] loss: 1.545, acc: 46.875
[2,  1395] loss: 1.529, acc: 46.875
[2,  1488] loss: 1.533, acc: 37.500
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[5, 1, 5, 0, 2, 5, 1, 2, 5, 2]
Acc: 41.4
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.4065678813639298
lr: 0.00075 lr2: 0.0
[3,    93] loss: 1.506, acc: 45.312
[3,   186] loss: 1.503, acc: 42.188
[3,   279] loss: 1.509, acc: 37.500
[3,   372] loss: 1.512, acc: 40.625
[3,   465] loss: 1.507, acc: 46.875
[3,   558] loss: 1.498, acc: 51.562
ad  a [3,   651] loss: 1.493, acc: 43.750
[3,   744] loss: 1.492, acc: 56.250
[3,   837] loss: 1.502, acc: 46.875
[3,   930] loss: 1.506, acc: 39.062
[3,  1023] loss: 1.478, acc: 45.312
[3,  1116] loss: 1.481, acc: 35.938
[3,  1209] loss: 1.488, acc: 48.438
[3,  1302] loss: 1.477, acc: 32.812
[3,  1395] loss: 1.492, acc: 46.875
[3,  1488] loss: 1.468, acc: 42.188
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[2, 1, 5, 0, 2, 5, 2, 1, 5, 2]
Acc: 41.9708333333
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.42109971462076334
lr: 0.0005625000000000001 lr2: 0.0002
[4,    93] loss: 1.469, acc: 43.750
q [4,   186] loss: 1.451, acc: 42.188
[4,   279] loss: 1.450, acc: 43.750
[4,   372] loss: 1.437, acc: 34.375
[4,   465] loss: 1.438, acc: 40.625
[4,   558] loss: 1.459, acc: 50.000
[4,   651] loss: 1.464, acc: 40.625
[4,   744] loss: 1.439, acc: 37.500
[4,   837] loss: 1.440, acc: 48.438
[4,   930] loss: 1.416, acc: 37.500
[4,  1023] loss: 1.426, acc: 35.938
[4,  1116] loss: 1.432, acc: 42.188
[4,  1209] loss: 1.402, acc: 54.688
[4,  1302] loss: 1.428, acc: 37.500
[4,  1395] loss: 1.421, acc: 40.625
[4,  1488] loss: 1.416, acc: 42.188
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[2, 1, 2, 0, 2, 5, 2, 1, 4, 2]
Acc: 44.4291666667
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.4397005976794613
avg_loss_weight: 
 0.4788
 0.6128
 0.6269
 0.8692
 0.9011
 0.4868
 0.3174
 0.8705
[torch.FloatTensor of size 8]

lr: 0.00042187500000000005 lr2: 0.00015000000000000001
[5,    93] loss: 1.360, acc: 43.750
^R
[5,   186] loss: 1.360, acc: 48.438
[5,   279] loss: 1.355, acc: 45.312
[5,   372] loss: 1.343, acc: 45.312
[5,   465] loss: 1.356, acc: 50.000
[5,   558] loss: 1.358, acc: 50.000
[5,   651] loss: 1.347, acc: 51.562
[5,   744] loss: 1.351, acc: 48.438
[5,   837] loss: 1.348, acc: 57.812
[5,   930] loss: 1.327, acc: 53.125
[5,  1023] loss: 1.362, acc: 53.125
q [5,  1116] loss: 1.350, acc: 48.438
[5,  1209] loss: 1.361, acc: 60.938
[5,  1302] loss: 1.358, acc: 42.188
[5,  1395] loss: 1.353, acc: 51.562
[5,  1488] loss: 1.349, acc: 54.688
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[2, 1, 5, 0, 1, 6, 2, 1, 4, 2]
Acc: 45.8916666667
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.4586881898146923
avg_loss_weight: 
 0.4797
 0.5706
 0.6607
 0.8884
 0.8319
 0.5065
 0.3158
 0.8728
[torch.FloatTensor of size 8]

lr: 0.00031640625000000006 lr2: 0.00011250000000000001
[6,    93] loss: 1.297, acc: 62.500
[6,   186] loss: 1.267, acc: 62.500
[6,   279] loss: 1.279, acc: 46.875
[6,   372] loss: 1.288, acc: 45.312
[6,   465] loss: 1.280, acc: 54.688
[6,   558] loss: 1.298, acc: 43.750
[6,   651] loss: 1.262, acc: 48.438
[6,   744] loss: 1.280, acc: 45.312
[6,   837] loss: 1.293, acc: 57.812
[6,   930] loss: 1.282, acc: 65.625
[6,  1023] loss: 1.297, acc: 64.062
[6,  1116] loss: 1.286, acc: 59.375
[6,  1209] loss: 1.281, acc: 54.688
[6,  1302] loss: 1.289, acc: 43.750
[6,  1395] loss: 1.277, acc: 53.125
[6,  1488] loss: 1.298, acc: 53.125
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[2, 2, 4, 0, 2, 6, 2, 1, 2, 2]
Acc: 45.0958333333
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.4554080407218286
avg_loss_weight: 
 0.4663
 0.5696
 0.6698
 0.8913
 0.8019
 0.5067
 0.3331
 0.8780
[torch.FloatTensor of size 8]

lr: 0.00023730468750000005 lr2: 8.4375e-05
[7,    93] loss: 1.248, acc: 51.562
[7,   186] loss: 1.223, acc: 48.438
[7,   279] loss: 1.251, acc: 60.938
[7,   372] loss: 1.223, acc: 54.688
[7,   465] loss: 1.204, acc: 53.125
[7,   558] loss: 1.233, acc: 43.750
[7,   651] loss: 1.231, acc: 53.125
[7,   744] loss: 1.213, acc: 42.188
^[[A^[[Aq[7,   837] loss: 1.229, acc: 54.688
[7,   930] loss: 1.237, acc: 51.562
[7,  1023] loss: 1.222, acc: 56.250
^[:q             [7,  1116] loss: 1.227, acc: 46.875
[7,  1209] loss: 1.234, acc: 65.625
[7,  1302] loss: 1.234, acc: 64.062
[7,  1395] loss: 1.220, acc: 48.438
[7,  1488] loss: 1.222, acc: 46.875
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[2, 2, 5, 0, 2, 6, 2, 1, 4, 2]
Acc: 45.5291666667
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.4590222971435652
avg_loss_weight: 
 0.4649
 0.5712
 0.6681
 0.8919
 0.7907
 0.4964
 0.3430
 0.8415
[torch.FloatTensor of size 8]

lr: 0.00017797851562500002 lr2: 6.328125e-05
[8,    93] loss: 1.188, acc: 59.375
[8,   186] loss: 1.170, acc: 46.875
[8,   279] loss: 1.188, acc: 62.500
[8,   372] loss: 1.184, acc: 54.688
[8,   465] loss: 1.157, acc: 48.438
[8,   558] loss: 1.166, acc: 62.500
[8,   651] loss: 1.176, acc: 57.812
[8,   744] loss: 1.198, acc: 59.375
[8,   837] loss: 1.162, acc: 68.750
[8,   930] loss: 1.200, acc: 53.125
[8,  1023] loss: 1.180, acc: 60.938
[8,  1116] loss: 1.166, acc: 51.562
[8,  1209] loss: 1.179, acc: 51.562
[8,  1302] loss: 1.170, acc: 50.000
[8,  1395] loss: 1.171, acc: 62.500
[8,  1488] loss: 1.179, acc: 50.000
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[4, 2, 4, 0, 2, 6, 2, 1, 3, 4]
Acc: 44.9833333333
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.457999883420886
avg_loss_weight: 
 0.4798
 0.5684
 0.6605
 0.8872
 0.7750
 0.5010
 0.3492
 0.8384
[torch.FloatTensor of size 8]

lr: 0.00013348388671875002 lr2: 4.74609375e-05
[9,    93] loss: 1.124, acc: 59.375
[9,   186] loss: 1.125, acc: 51.562
[9,   279] loss: 1.122, acc: 59.375
[9,   372] loss: 1.112, acc: 53.125
[9,   465] loss: 1.138, acc: 59.375
[9,   558] loss: 1.145, acc: 57.812
[9,   651] loss: 1.129, acc: 62.500
[9,   744] loss: 1.128, acc: 57.812
[9,   837] loss: 1.148, acc: 56.250
[9,   930] loss: 1.146, acc: 56.250
[9,  1023] loss: 1.136, acc: 59.375
[9,  1116] loss: 1.155, acc: 42.188
[9,  1209] loss: 1.142, acc: 48.438
[9,  1302] loss: 1.141, acc: 54.688
[9,  1395] loss: 1.147, acc: 50.000
[9,  1488] loss: 1.139, acc: 64.062
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[4, 2, 5, 0, 2, 6, 2, 1, 4, 2]
Acc: 44.6041666667
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.4542862844189264
avg_loss_weight: 
 0.4888
 0.5740
 0.6562
 0.8847
 0.7652
 0.5011
 0.3523
 0.8378
[torch.FloatTensor of size 8]

lr: 0.00010011291503906251 lr2: 3.5595703124999996e-05
[10,    93] loss: 1.105, acc: 51.562
[10,   186] loss: 1.108, acc: 56.250
[10,   279] loss: 1.101, acc: 65.625
[10,   372] loss: 1.110, acc: 59.375
[10,   465] loss: 1.124, acc: 56.250
[10,   558] loss: 1.092, acc: 60.938
[10,   651] loss: 1.106, acc: 65.625
[10,   744] loss: 1.103, acc: 64.062
[10,   837] loss: 1.095, acc: 62.500
[10,   930] loss: 1.081, acc: 51.562
[10,  1023] loss: 1.095, acc: 62.500
[10,  1116] loss: 1.080, acc: 57.812
[10,  1209] loss: 1.110, acc: 68.750
^CProcess Process-76:
Process Process-73:
Process Process-75:
Process Process-74:
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/process.py", line 249, in _bootstrap
    self.run()
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/process.py", line 249, in _bootstrap
    self.run()
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/process.py", line 249, in _bootstrap
    self.run()
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 34, in _worker_loop
    r = index_queue.get()
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 34, in _worker_loop
    r = index_queue.get()
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 34, in _worker_loop
    r = index_queue.get()
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/queues.py", line 342, in get
    with self._rlock:
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/queues.py", line 342, in get
    with self._rlock:
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/synchronize.py", line 96, in __enter__
    return self._semlock.__enter__()
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/queues.py", line 342, in get
    with self._rlock:
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/synchronize.py", line 96, in __enter__
    return self._semlock.__enter__()
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/synchronize.py", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
KeyboardInterrupt
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/process.py", line 249, in _bootstrap
    self.run()
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 34, in _worker_loop
    r = index_queue.get()
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/queues.py", line 343, in get
    res = self._reader.recv_bytes()
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Traceback (most recent call last):
  File "train.py", line 214, in <module>
    main(args.model_id, use_element, args.is_save)
  File "train.py", line 152, in main
    loss.backward()
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/autograd/variable.py", line 156, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/autograd/__init__.py", line 98, in backward
    variables, grad_variables, retain_graph)
KeyboardInterrupt
dyhu@mit116:~/BDCI2017-MingLue$ exit
exit
