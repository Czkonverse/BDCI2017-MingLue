dyhu@mit116:~/BDCI2017-MingLue/log/HAN-11-28-13:20/HAN-11-30$ [Kdyhu@mit116:~/BDCI2017-MingLue/log/HAN-11-28-13:20/HAN-11-30$ cd ../..
dyhu@mit116:~/BDCI2017-MingLue/log$ cd ..
dyhu@mit116:~/BDCI2017-MingLue$ p[KCUDA_VISIBLE_DEVICE[K[KCES+[K=2  [K[K H[Kpython train.py --model-id 4
Using TensorFlow backend.
loading data...
max sentence length:  42583
total vocab size 691360
load word2index
0 1

 (120000, 40, 60)
data loaded
config vocab size: 194170
pretrain...
loss weight: 
 0.7786
 0.8593
 1.1661
 1.2979
 1.1405
 0.9833
 0.5955
 1.1791
[torch.FloatTensor of size 8]

training...
lr: 0.001 lr2: 0.0
Traceback (most recent call last):
  File "train.py", line 214, in <module>
    main(args.model_id, use_element, args.is_save)
  File "train.py", line 150, in main
    outputs = model(inputs)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/nn/modules/module.py", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/dyhu/BDCI2017-MingLue/models/hierarchical.py", line 248, in forward
    x = self.word_to_sentence(x, word_hidden_stat, sequence_lens)  # [batch_size * num_sentences, word_hidden_size*2]
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/nn/modules/module.py", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/dyhu/BDCI2017-MingLue/models/hierarchical.py", line 76, in forward
    projection = self.word_proj_nonlinearity(self.bn(self.word_projection(output))
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/nn/modules/module.py", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 37, in forward
    self.training, self.momentum, self.eps)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/nn/functional.py", line 639, in batch_norm
    return f(input, weight, bias)
RuntimeError: running_mean should contain 60 elements not 100
dyhu@mit116:~/BDCI2017-MingLue$ 
dyhu@mit116:~/BDCI2017-MingLue$  CUDA_VISIBLE_DEVICES=2 python train.py --model-id 4[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ccd ..[K/..less screenlog.0 cd HAN-11-28-13\:20/BDCI2017-MingLue/log/[7Pscreen -r lhlilu ls[Kls[Khtop [6@nvidia-smi[C[27@less log/HAN-11-28-13\:20/screenlog.0[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[12Pscreen -r HAN-11-28-13:2[C[C[9Plhlilu[C[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Kdyhu@mit116:~/BDCI2017-MingLue$ screen -lsls[Kess log/rcnn-11-29/screenlog.0 s[Kess log/rcnn-11-29/screenlog.0 htop[Kscreen -lsr lhlilu [3Pvim config.py[C[K[K[K[K[K[K[K[K[K[K[K[K[K[KCUd[K[Khtopnvidia-smi [6@screen -r lhlilu[Cls[Knvidia-smi [21@less log/rcnn-11-29/screenlog.0[Chtop[Kless log/rcnn-11-29/screenlog.0 s[Kscreen -r lhlilu ls[Kvim config.py screen -S lhliluls[Kpyenv activate private [12Pnvidia-smi[C[7Phtopless log/HAN-11-28-13\:20/screenlog.0 [K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[KCUDA_VISIBE[KLE_DEVICES=2 python train.py --model-id 4
Using TensorFlow backend.
Traceback (most recent call last):
  File "train.py", line 22, in <module>
    from utils.trainhelper import accuracy, model_selector, do_eval, build_element_vec
  File "/home/dyhu/BDCI2017-MingLue/utils/trainhelper.py", line 9, in <module>
    from models.hierarchical import HAN
  File "/home/dyhu/BDCI2017-MingLue/models/hierarchical.py", line 81
    ).view(-1, self.word_context_size)  # [2x3, 5]
    ^
SyntaxError: invalid syntax
dyhu@mit116:~/BDCI2017-MingLue$ CUDA_VISIBLE_DEVICES=2 python train.py --model-id 4[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ccd ..[K/..less screenlog.0 cd HAN-11-28-13\:20/BDCI2017-MingLue/log/[Kdyhu@mit116:~/BDCI2017-MingLue$ cd BDCI2017-MingLue/log/[4PHAN-11-28-13\:20/[3Pless screenlog.0 cd HAN-11-28-13\:20/BDCI2017-MingLue/log/[7Pscreen -r lhlilu cd BDCI2017-MingLue/log/[4PHAN-11-28-13\:20/[3Pless screenlog.0 [9Pcd ../..[KCUDA_VISIBLE_DEVICES=2 python train.py --model-id 4
Using TensorFlow backend.
Traceback (most recent call last):
  File "train.py", line 22, in <module>
    from utils.trainhelper import accuracy, model_selector, do_eval, build_element_vec
  File "/home/dyhu/BDCI2017-MingLue/utils/trainhelper.py", line 9, in <module>
    from models.hierarchical import HAN
  File "/home/dyhu/BDCI2017-MingLue/models/hierarchical.py", line 82
    attention = torch.mm(projection, self.word_context)  # [2x3, 1]
            ^
SyntaxError: invalid syntax
dyhu@mit116:~/BDCI2017-MingLue$ CUDA_VISIBLE_DEVICES=2 python train.py --model-id 4
Using TensorFlow backend.
loading data...
max sentence length:  42583
total vocab size 691360
load word2index
0 1
(120000, 40, 60)
data loaded
config vocab size: 194170
pretrain...
loss weight: 
 0.7786
 0.8593
 1.1661
 1.2979
 1.1405
 0.9833
 0.5955
 1.1791
[torch.FloatTensor of size 8]

training...
lr: 0.001 lr2: 0.0
torch.Size([2560, 60, 100])
Traceback (most recent call last):
  File "train.py", line 214, in <module>
    main(args.model_id, use_element, args.is_save)
  File "train.py", line 150, in main
    outputs = model(inputs)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/nn/modules/module.py", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/dyhu/BDCI2017-MingLue/models/hierarchical.py", line 252, in forward
    x = self.word_to_sentence(x, word_hidden_stat, sequence_lens)  # [batch_size * num_sentences, word_hidden_size*2]
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/nn/modules/module.py", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/dyhu/BDCI2017-MingLue/models/hierarchical.py", line 78, in forward
    projection = self.bn(projection)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/nn/modules/module.py", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 37, in forward
    self.training, self.momentum, self.eps)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/nn/functional.py", line 639, in batch_norm
    return f(input, weight, bias)
RuntimeError: running_mean should contain 60 elements not 100
dyhu@mit116:~/BDCI2017-MingLue$ CUDA_VISIBLE_DEVICES=2 python train.py --model-id 4
Using TensorFlow backend.
loading data...
max sentence length:  42583
total vocab size 691360
load word2index
0 1
(120000, 40, 60)
data loaded
config vocab size: 194170
pretrain...
loss weight: 
 0.7786
 0.8593
 1.1661
 1.2979
 1.1405
 0.9833
 0.5955
 1.1791
[torch.FloatTensor of size 8]

training...
lr: 0.001 lr2: 0.0
[1,    93] loss: 1.914, acc: 31.250
[1,   186] loss: 1.781, acc: 29.688
[1,   279] loss: 1.726, acc: 34.375
[1,   372] loss: 1.709, acc: 31.250
[1,   465] loss: 1.667, acc: 34.375
[1,   558] loss: 1.661, acc: 46.875
[1,   651] loss: 1.634, acc: 37.500
[1,   744] loss: 1.631, acc: 40.625
[1,   837] loss: 1.629, acc: 32.812
[1,   930] loss: 1.597, acc: 46.875
[1,  1023] loss: 1.611, acc: 37.500
[1,  1116] loss: 1.584, acc: 32.812
[1,  1209] loss: 1.575, acc: 51.562
[1,  1302] loss: 1.563, acc: 50.000
[1,  1395] loss: 1.542, acc: 35.938
[1,  1488] loss: 1.531, acc: 43.750
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[4, 1, 5, 0, 1, 5, 1, 1, 5, 1]
Acc: 42.3125
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.3954789255966062
lr: 0.001 lr2: 0.0
[2,    93] loss: 1.535, acc: 46.875
[2,   186] loss: 1.528, acc: 42.188
[2,   279] loss: 1.500, acc: 45.312
[2,   372] loss: 1.505, acc: 45.312
[2,   465] loss: 1.519, acc: 40.625
[2,   558] loss: 1.490, acc: 48.438
[2,   651] loss: 1.502, acc: 48.438
[2,   744] loss: 1.480, acc: 39.062
[2,   837] loss: 1.481, acc: 46.875
[2,   930] loss: 1.478, acc: 40.625
[2,  1023] loss: 1.469, acc: 46.875
[2,  1116] loss: 1.471, acc: 45.312
[2,  1209] loss: 1.460, acc: 35.938
[2,  1302] loss: 1.440, acc: 39.062
[2,  1395] loss: 1.439, acc: 50.000
[2,  1488] loss: 1.438, acc: 39.062
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[4, 1, 1, 0, 4, 5, 2, 1, 4, 1]
Acc: 45.1416666667
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.44242261450168424
lr: 0.00075 lr2: 0.0
[3,    93] loss: 1.400, acc: 56.250
[3,   186] loss: 1.415, acc: 48.438
[3,   279] loss: 1.407, acc: 45.312
[3,   372] loss: 1.385, acc: 46.875
[3,   465] loss: 1.395, acc: 50.000
[3,   558] loss: 1.406, acc: 46.875
[3,   651] loss: 1.398, acc: 45.312
[3,   744] loss: 1.379, acc: 42.188
[3,   837] loss: 1.391, acc: 53.125
[3,   930] loss: 1.395, acc: 46.875
[3,  1023] loss: 1.399, acc: 45.312
[3,  1116] loss: 1.400, acc: 45.312
[3,  1209] loss: 1.376, acc: 45.312
[3,  1302] loss: 1.391, acc: 54.688
[3,  1395] loss: 1.393, acc: 48.438
[3,  1488] loss: 1.384, acc: 51.562
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[5, 2, 1, 0, 4, 6, 2, 1, 4, 1]
Acc: 45.8958333333
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.45236721296429633
lr: 0.0005625000000000001 lr2: 0.0002
[4,    93] loss: 1.343, acc: 56.250
[4,   186] loss: 1.349, acc: 43.750
[4,   279] loss: 1.361, acc: 54.688
[4,   372] loss: 1.337, acc: 51.562
[4,   465] loss: 1.328, acc: 45.312
[4,   558] loss: 1.335, acc: 46.875
[4,   651] loss: 1.323, acc: 48.438
[4,   744] loss: 1.330, acc: 54.688
[4,   837] loss: 1.327, acc: 54.688
[4,   930] loss: 1.327, acc: 53.125
[4,  1023] loss: 1.350, acc: 54.688
[4,  1116] loss: 1.313, acc: 46.875
[4,  1209] loss: 1.333, acc: 59.375
[4,  1302] loss: 1.338, acc: 59.375
[4,  1395] loss: 1.324, acc: 54.688
[4,  1488] loss: 1.321, acc: 48.438
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[5, 2, 1, 0, 1, 6, 4, 1, 4, 2]
Acc: 48.4166666667
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.47617610284029177
avg_loss_weight: 
 0.4822
 0.4453
 0.7279
 0.8741
 0.7785
 0.5249
 0.2515
 0.8527
[torch.FloatTensor of size 8]

lr: 0.00042187500000000005 lr2: 0.00015000000000000001
^[[A^[[A^[[A^[[A[5,    93] loss: 1.231, acc: 67.188
[5,   186] loss: 1.233, acc: 57.812
[5,   279] loss: 1.223, acc: 43.750
[5,   372] loss: 1.241, acc: 57.812
[5,   465] loss: 1.216, acc: 64.062
[5,   558] loss: 1.237, acc: 57.812
[5,   651] loss: 1.243, acc: 56.250
[5,   744] loss: 1.223, acc: 53.125
[5,   837] loss: 1.225, acc: 45.312
[5,   930] loss: 1.246, acc: 56.250
[5,  1023] loss: 1.231, acc: 60.938
[5,  1116] loss: 1.218, acc: 56.250
[5,  1209] loss: 1.225, acc: 53.125
[5,  1302] loss: 1.226, acc: 53.125
[5,  1395] loss: 1.227, acc: 54.688
[5,  1488] loss: 1.210, acc: 59.375
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[5, 2, 1, 0, 0, 6, 2, 1, 4, 2]
Acc: 47.225
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.4739030308016513
avg_loss_weight: 
 0.4431
 0.5014
 0.6812
 0.8643
 0.7867
 0.4876
 0.3045
 0.8103
[torch.FloatTensor of size 8]

lr: 0.00031640625000000006 lr2: 0.00011250000000000001
[6,    93] loss: 1.144, acc: 50.000
[6,   186] loss: 1.128, acc: 53.125
[6,   279] loss: 1.137, acc: 68.750
[6,   372] loss: 1.136, acc: 48.438
[6,   465] loss: 1.123, acc: 54.688
[6,   558] loss: 1.125, acc: 51.562
[6,   651] loss: 1.132, acc: 65.625
[6,   744] loss: 1.145, acc: 57.812
[6,   837] loss: 1.129, acc: 65.625
[6,   930] loss: 1.135, acc: 65.625
[6,  1023] loss: 1.127, acc: 60.938
[6,  1116] loss: 1.132, acc: 59.375
[6,  1209] loss: 1.112, acc: 51.562
[6,  1302] loss: 1.132, acc: 70.312
[6,  1395] loss: 1.122, acc: 57.812
[6,  1488] loss: 1.121, acc: 64.062
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[4, 2, 1, 0, 0, 6, 4, 1, 4, 2]
Acc: 46.3458333333
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.472598713111656
avg_loss_weight: 
 0.4591
 0.5154
 0.6501
 0.8659
 0.7522
 0.5148
 0.3225
 0.7827
[torch.FloatTensor of size 8]

lr: 0.00023730468750000005 lr2: 8.4375e-05
[7,    93] loss: 1.029, acc: 67.188
[7,   186] loss: 1.037, acc: 51.562
[7,   279] loss: 1.028, acc: 57.812
[7,   372] loss: 1.033, acc: 73.438
[7,   465] loss: 1.029, acc: 67.188
[7,   558] loss: 1.028, acc: 68.750
[7,   651] loss: 1.032, acc: 68.750
[7,   744] loss: 1.011, acc: 65.625
[7,   837] loss: 1.031, acc: 64.062
[7,   930] loss: 1.010, acc: 62.500
[7,  1023] loss: 1.024, acc: 71.875
[7,  1116] loss: 1.038, acc: 56.250
[7,  1209] loss: 1.041, acc: 60.938
[7,  1302] loss: 1.036, acc: 64.062
[7,  1395] loss: 1.028, acc: 60.938
[7,  1488] loss: 1.048, acc: 59.375
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[5, 2, 1, 0, 2, 6, 4, 1, 4, 2]
Acc: 46.0875
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.47148403206627487
avg_loss_weight: 
 0.4612
 0.5180
 0.6521
 0.8516
 0.7365
 0.5146
 0.3418
 0.7913
[torch.FloatTensor of size 8]

lr: 0.00017797851562500002 lr2: 6.328125e-05
[8,    93] loss: 0.942, acc: 68.750
[8,   186] loss: 0.933, acc: 75.000
[8,   279] loss: 0.943, acc: 60.938
[8,   372] loss: 0.941, acc: 71.875
[8,   465] loss: 0.939, acc: 60.938
[8,   558] loss: 0.938, acc: 76.562
[8,   651] loss: 0.956, acc: 73.438
[8,   744] loss: 0.927, acc: 71.875
[8,   837] loss: 0.975, acc: 57.812
[8,   930] loss: 0.947, acc: 67.188
[8,  1023] loss: 0.927, acc: 64.062
[8,  1116] loss: 0.915, acc: 57.812
[8,  1209] loss: 0.941, acc: 67.188
[8,  1302] loss: 0.957, acc: 70.312
[8,  1395] loss: 0.945, acc: 71.875
[8,  1488] loss: 0.931, acc: 64.062
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[4, 2, 1, 0, 2, 6, 4, 1, 4, 2]
Acc: 45.8916666667
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.4682916768368684
avg_loss_weight: 
 0.4621
 0.5272
 0.6564
 0.8432
 0.7259
 0.5171
 0.3474
 0.7902
[torch.FloatTensor of size 8]

lr: 0.00013348388671875002 lr2: 4.74609375e-05
[9,    93] loss: 0.872, acc: 60.938
[9,   186] loss: 0.860, acc: 73.438
[9,   279] loss: 0.855, acc: 76.562
[9,   372] loss: 0.885, acc: 78.125
[9,   465] loss: 0.846, acc: 70.312
[9,   558] loss: 0.869, acc: 75.000
[9,   651] loss: 0.881, acc: 73.438
[9,   744] loss: 0.840, acc: 65.625
[9,   837] loss: 0.872, acc: 76.562
[9,   930] loss: 0.858, acc: 68.750
[9,  1023] loss: 0.859, acc: 81.250
[9,  1116] loss: 0.859, acc: 71.875
[9,  1209] loss: 0.865, acc: 67.188
[9,  1302] loss: 0.859, acc: 76.562
[9,  1395] loss: 0.878, acc: 64.062
[9,  1488] loss: 0.850, acc: 64.062
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[4, 2, 1, 0, 2, 6, 4, 1, 4, 2]
Acc: 45.675
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.46661895488226357
avg_loss_weight: 
 0.4616
 0.5365
 0.6554
 0.8325
 0.7195
 0.5225
 0.3513
 0.7894
[torch.FloatTensor of size 8]

lr: 0.00010011291503906251 lr2: 3.5595703124999996e-05
[10,    93] loss: 0.799, acc: 81.250
[10,   186] loss: 0.790, acc: 65.625
[10,   279] loss: 0.816, acc: 78.125
[10,   372] loss: 0.792, acc: 81.250
[10,   465] loss: 0.793, acc: 75.000
[10,   558] loss: 0.799, acc: 76.562
[10,   651] loss: 0.803, acc: 68.750
[10,   744] loss: 0.803, acc: 75.000
[10,   837] loss: 0.796, acc: 84.375
[10,   930] loss: 0.799, acc: 70.312
[10,  1023] loss: 0.791, acc: 73.438
[10,  1116] loss: 0.802, acc: 71.875
[10,  1209] loss: 0.791, acc: 71.875
[10,  1302] loss: 0.803, acc: 73.438
[10,  1395] loss: 0.808, acc: 67.188
[10,  1488] loss: 0.799, acc: 75.000
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[5, 2, 1, 0, 2, 6, 4, 1, 4, 3]
Acc: 45.3375
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.46408456694088873
avg_loss_weight: 
 0.4670
 0.5409
 0.6536
 0.8282
 0.7159
 0.5252
 0.3547
 0.7895
[torch.FloatTensor of size 8]

lr: 7.508468627929689e-05 lr2: 2.6696777343749997e-05
[11,    93] loss: 0.747, acc: 70.312
[11,   186] loss: 0.747, acc: 70.312
[11,   279] loss: 0.756, acc: 78.125
[11,   372] loss: 0.736, acc: 75.000
[11,   465] loss: 0.750, acc: 75.000
[11,   558] loss: 0.737, acc: 71.875
[11,   651] loss: 0.746, acc: 75.000
[11,   744] loss: 0.749, acc: 70.312
[11,   837] loss: 0.762, acc: 68.750
[11,   930] loss: 0.749, acc: 71.875
[11,  1023] loss: 0.741, acc: 75.000
[11,  1116] loss: 0.749, acc: 76.562
[11,  1209] loss: 0.754, acc: 84.375
[11,  1302] loss: 0.748, acc: 79.688
[11,  1395] loss: 0.748, acc: 75.000
[11,  1488] loss: 0.734, acc: 70.312
predicting...
[2, 1, 1, 0, 2, 6, 6, 1, 4, 4]
[4, 2, 1, 0, 2, 6, 4, 0, 4, 2]
Acc: 45.1875
Counter({6: 5869, 1: 4060, 5: 3835, 2: 2999, 0: 2928, 4: 2862, 3: 1223, 7: 224})
Micro-Averaged F1: 0.46140379497598877
avg_loss_weight: 
 0.4641
 0.5493
 0.6533
 0.8240
 0.7111
 0.5298
 0.3570
 0.7885
[torch.FloatTensor of size 8]

lr: 5.631351470947266e-05 lr2: 2.0022583007812497e-05
[12,    93] loss: 0.735, acc: 70.312
[12,   186] loss: 0.693, acc: 82.812
[12,   279] loss: 0.693, acc: 75.000
[12,   372] loss: 0.710, acc: 78.125
[12,   465] loss: 0.703, acc: 85.938
[12,   558] loss: 0.720, acc: 73.438
^CProcess Process-92:
Process Process-90:
Process Process-89:
Process Process-91:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/process.py", line 249, in _bootstrap
    self.run()
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 34, in _worker_loop
    r = index_queue.get()
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/queues.py", line 342, in get
    with self._rlock:
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/process.py", line 249, in _bootstrap
    self.run()
Traceback (most recent call last):
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/synchronize.py", line 96, in __enter__
    return self._semlock.__enter__()
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 34, in _worker_loop
    r = index_queue.get()
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/queues.py", line 342, in get
    with self._rlock:
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/synchronize.py", line 96, in __enter__
    return self._semlock.__enter__()
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/process.py", line 249, in _bootstrap
    self.run()
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 34, in _worker_loop
    r = index_queue.get()
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/queues.py", line 342, in get
    with self._rlock:
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/synchronize.py", line 96, in __enter__
    return self._semlock.__enter__()
Traceback (most recent call last):
  File "train.py", line 214, in <module>
KeyboardInterrupt
KeyboardInterrupt
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/process.py", line 249, in _bootstrap
    self.run()
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/dyhu/.pyenv/versions/private/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 34, in _worker_loop
    r = index_queue.get()
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/queues.py", line 343, in get
    res = self._reader.recv_bytes()
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/home/dyhu/.pyenv/versions/3.6.1/lib/python3.6/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
    main(args.model_id, use_element, args.is_save)
  File "train.py", line 155, in main
    running_loss += loss.data[0]
KeyboardInterrupt
exidyhu@mit116:~/BDCI2017-MingLue$ exit
exit
